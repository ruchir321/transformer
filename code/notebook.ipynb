{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpeeYR3h2EFo"
      },
      "source": [
        "# Wikihow GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"../\")\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "# import tiktoken # Not used, smallest embedding table is too large for potato laptop compute\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zVQsVaO8dosQ"
      },
      "outputs": [],
      "source": [
        "def get_sample(filepath, size=1024 *1024):\n",
        "  with open(filepath, 'r') as f:\n",
        "    sample = f.read(size)\n",
        "  return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m elval_intervals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[1;32m      6\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[0;32m----> 7\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m eval_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m      9\u001b[0m n_embd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m \u001b[38;5;66;03m# aka d_model?? (vaswani 2017)\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "context_window = 128 # this is what karpathy calls block size in his code\n",
        "max_iters = 6000 \n",
        "elval_intervals = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32 # aka d_model?? (vaswani 2017)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBTzqKMw2Yo9",
        "outputId": "2becc0f9-211c-45e4-93e7-e8012c634f63"
      },
      "outputs": [],
      "source": [
        "# wikihow corpus\n",
        "txt = get_sample('data/wikihow.txt')\n",
        "print(len(txt)) # 621,684,876 characters\n",
        "print(txt[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# tokenization\n",
        "\n",
        "trade-off: Vocab-size VS. token sequence length\n",
        "\n",
        "`200k-4o` embedding vocab size of 200k is too large for my potato laptop\n",
        "\n",
        "character-wise embeddings are manageable enough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ2XvB8I3n83",
        "outputId": "15a53df1-1b62-4618-cb88-aa8145fb79e2"
      },
      "outputs": [],
      "source": [
        "char_set = sorted(set(txt)) # character-wise tokenization\n",
        "vocab_size = len(char_set)\n",
        "print(len(char_set)) # 95 characters\n",
        "print(char_set[-20:])\n",
        "\n",
        "# encoder and decoder mappings and functions\n",
        "encode_mapping = {t: i for i, t in enumerate(sorted(set(txt)))}\n",
        "decode_mapping = {v: k for k, v in encode_mapping.items()}\n",
        "\n",
        "encode = lambda s: [encode_mapping[t] for t in s]\n",
        "decode = lambda tok: ''.join([decode_mapping[t] for t in tok])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens = encode(txt)\n",
        "print(len(tokens))\n",
        "print(tokens[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d73Q1G2V51AN",
        "outputId": "24032cc5-3076-4ccd-ae67-83651b0ca24d"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor(data=tokens,dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gqO1Bl27CSb"
      },
      "outputs": [],
      "source": [
        "train_test_split_index = int(0.9*(len(data)))\n",
        "print(train_test_split_index)\n",
        "print(type(train_test_split_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0rmkoSC468Mo"
      },
      "outputs": [],
      "source": [
        "train = data[:train_test_split_index]\n",
        "test = data[train_test_split_index:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHcqkNQh7Y-p"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "context_window = 128 # this is what karpathy calls block size in his code\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ov7p8nTpLH-b"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "  \"\"\"Return a batch of x and y tensors of shape (batch_size, context_window)\"\"\"\n",
        "  data = train if split == \"train\" else test\n",
        "  ix = torch.randint(high=(len(data) - context_window), size=(batch_size,)) # get random indices for context sequences\n",
        "  x = torch.stack([data[i:i+context_window] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+context_window+1] for i in ix])\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kq1zEvmkS7hg"
      },
      "outputs": [],
      "source": [
        "xb, yb = get_batch(\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T4km-ZCWevH",
        "outputId": "fef1c431-0aca-4ff6-c5e5-57e9c7c2b56b"
      },
      "outputs": [],
      "source": [
        "print(xb.shape)\n",
        "print(yb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h9l1QlgWifc"
      },
      "source": [
        "## baseline model\n",
        "\n",
        "Bigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_embd = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BigramModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # each token simply picks the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # aka input embeddings (vaswani 2017) \n",
        "    self.positional_embedding_table = nn.Embedding(context_window, n_embd)\n",
        "    # conversion from token embeddings to logits\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    token_embd = self.token_embedding_table(idx) # (B, T, C) # x tokens are used as indices of embedding table\n",
        "    pos_embd = self.positional_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "    x = token_embd + pos_embd # (B, T, C)\n",
        "    # query: what am i looking for\n",
        "    # key: what do i contain\n",
        "\n",
        "    logits = self.lm_head(x)  \n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    \n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    \n",
        "    return logits, loss\n",
        "  \n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get predictions\n",
        "      logits, loss = self.forward(idx)\n",
        "\n",
        "      # last time step element contains the prediction\n",
        "      logits = logits[:,-1, :] # becomes (B, C) shape\n",
        "\n",
        "      # convert to probabilities\n",
        "      probs = F.softmax(logits, dim=1) # (B, C)\n",
        "\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "      # append sample to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T + 1)\n",
        "    return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bigram = BigramModel(vocab_size=vocab_size)\n",
        "logits, loss = bigram.forward(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bigram.token_embedding_table.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long) # start with token 0 (i.e. \\u character)\n",
        "gen = bigram.generate(idx, max_new_tokens=100)\n",
        "print(gen.shape) # for a batch of 100 token sequence, we get a 101 length sequence in return where 101-th element is the prediction\n",
        "print(gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction = decode(gen[0].tolist())\n",
        "print(prediction) # the model predicts garbage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for param in bigram.parameters():\n",
        "    print(type(param))\n",
        "    print(param.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# optimizer\n",
        "optimizer = torch.optim.AdamW(bigram.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# iterate over multiple batches\n",
        "batch_size = 32 # use bigger batch size\n",
        "steps = 10000\n",
        "\n",
        "for _ in range(steps):\n",
        "\n",
        "    # get a batch of x and y\n",
        "    xb, yb = get_batch(\"train\")\n",
        "\n",
        "    # evaluate loss\n",
        "    logits, loss = bigram(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True) # reset gradients from previous step to zero\n",
        "    loss.backward() # calc gradients for all the parameters\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long) # start with token 0 (i.e. \\u character)\n",
        "gen = bigram.generate(idx, max_new_tokens=100)\n",
        "prediction = decode(gen[0].tolist())\n",
        "print(prediction) # the model predicts garbage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "genAI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
